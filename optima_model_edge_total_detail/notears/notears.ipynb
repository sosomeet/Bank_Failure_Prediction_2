{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32679d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEARS → Edge Sweep(k=1..Kmax) → F(m/mw) → O/F/OF × 10-fold CV\n",
    "# Models: Logit, RF, XGBoost(있으면), LightGBM(있으면)\n",
    "# Outputs:\n",
    "#   - logs/run_log.csv\n",
    "#   - reports/report_long.csv\n",
    "#   - reports/report_pivot.csv\n",
    "#   - reports/report_all_like_user.csv\n",
    "#   - reports/best_per_model.csv\n",
    "#   - reports/edges/master_edges.csv\n",
    "#   - reports/edges/m_k{K}.csv, reports/edges/mw_k{K}.csv  (각 k마다 사용 엣지 기록)\n",
    "\n",
    "import warnings, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# optional\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    _HAS_LGBM = True\n",
    "except Exception:\n",
    "    _HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    _HAS_XGB = True\n",
    "except Exception:\n",
    "    _HAS_XGB = False\n",
    "\n",
    "# gCastle NOTEARS (미설치여도 notears_linear로 진행)\n",
    "try:\n",
    "    from castle.algorithms import Notears  # noqa: F401\n",
    "    _HAS_NOTEARS = True\n",
    "except Exception:\n",
    "    _HAS_NOTEARS = False\n",
    "    warnings.warn(\"gCastle 미설치: notears_linear만 사용합니다.\")\n",
    "\n",
    "# 기본 설정\n",
    "DATA_PATH = \"./training_data.csv\"\n",
    "LABEL_COL = \"label\"\n",
    "ID_PREFIX = \"Unnamed\"\n",
    "N_SPLITS = 10\n",
    "RANDOM_STATE = 42\n",
    "DAG_NAME = \"NOTEARS\"\n",
    "CLASS_WEIGHT_BALANCED = True\n",
    "VERBOSE_PRINT = True  # 학습 1회당 요약 1줄\n",
    "\n",
    "# 모델 파라미터\n",
    "LGBM_PARAMS = dict(\n",
    "    n_estimators=300, learning_rate=0.05, num_leaves=63, max_depth=-1,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, reg_alpha=0.0,\n",
    "    objective=\"binary\", class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_lambda=1.0, reg_alpha=0.0, objective=\"binary:logistic\", eval_metric=\"auc\",\n",
    "    random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 유틸/평가/모델팩토리/저장\n",
    "# ---------------------------\n",
    "\n",
    "def ensure_dirs():\n",
    "    Path(\"./logs\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"./reports\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"./reports/edges\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    bins = np.linspace(0,1,n_bins+1); ece=0.0; N=len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        L,R = bins[b], bins[b+1]\n",
    "        m = (y_prob>=L)&(y_prob<(R if b<n_bins-1 else R+1e-12))\n",
    "        if not np.any(m): continue\n",
    "        ece += (m.sum()/N)*abs(y_true[m].mean()-y_prob[m].mean())\n",
    "    return float(ece)\n",
    "\n",
    "def run_cv(model, X, y, n_splits=10, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    f1s, auprcs, aurocs, briers, eces = [], [], [], [], []\n",
    "    Xv = X.values.astype(np.float32); yv = y.values.astype(int)\n",
    "    for tr, te in skf.split(Xv, yv):\n",
    "        X_tr, X_te, y_tr, y_te = Xv[tr], Xv[te], yv[tr], yv[te]\n",
    "        is_lgbm = (model.__class__.__name__ == \"LGBMClassifier\")\n",
    "        if is_lgbm:\n",
    "            X_tr2, X_val, y_tr2, y_val = train_test_split(\n",
    "                X_tr, y_tr, test_size=0.2, random_state=seed, stratify=y_tr\n",
    "            )\n",
    "            model.fit(\n",
    "                X_tr2, y_tr2,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"auc\"\n",
    "                # 필요시: early_stopping_rounds=50\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "        proba = model.predict_proba(X_te)[:,1] if hasattr(model,\"predict_proba\") else model.predict(X_te).astype(float)\n",
    "        pred = (proba>=0.5).astype(int)\n",
    "        f1s.append(f1_score(y_te, pred, zero_division=0))\n",
    "        auprcs.append(average_precision_score(y_te, proba) if len(np.unique(y_te))>1 else np.nan)\n",
    "        aurocs.append(roc_auc_score(y_te, proba) if len(np.unique(y_te))>1 else np.nan)\n",
    "        briers.append(brier_score_loss(y_te, proba))\n",
    "        eces.append(expected_calibration_error(y_te, proba, 10))\n",
    "    return dict(f1=float(np.nanmean(f1s)), AUPRC=float(np.nanmean(auprcs)), AUROC=float(np.nanmean(aurocs)),\n",
    "                Brier=float(np.nanmean(briers)), ECE=float(np.nanmean(eces)))\n",
    "\n",
    "def make_models():\n",
    "    ms=[(\"Logit\", LogisticRegression(solver=\"lbfgs\", max_iter=200,\n",
    "                                     class_weight=(\"balanced\" if CLASS_WEIGHT_BALANCED else None))),\n",
    "        (\"RandomForest\", RandomForestClassifier(n_estimators=400,\n",
    "                                               class_weight=(\"balanced\" if CLASS_WEIGHT_BALANCED else None),\n",
    "                                               n_jobs=-1, random_state=RANDOM_STATE))]\n",
    "    if _HAS_XGB:\n",
    "        ms.append((\"XGBoost\", XGBClassifier(**XGB_PARAMS, verbosity=0)))\n",
    "    if _HAS_LGBM:\n",
    "        ms.append((\"LightGBM\", LGBMClassifier(**LGBM_PARAMS)))\n",
    "    return ms\n",
    "\n",
    "def append_log_csv(row: Dict):\n",
    "    ensure_dirs()\n",
    "    p=Path(\"./logs/run_log.csv\")\n",
    "    pd.DataFrame([row]).to_csv(p, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=not p.exists())\n",
    "\n",
    "def one_line(dag,k,model,set_name,met,secs):\n",
    "    return (f\"DAG={dag} | k={k} | model={model} | set={set_name} | \"\n",
    "            f\"f1={met['f1']:.4f}, AUPRC={met['AUPRC']:.4f}, AUROC={met['AUROC']:.4f}, \"\n",
    "            f\"Brier={met['Brier']:.4f}, ECE={met['ECE']:.4f} | time={secs:.2f}s\")\n",
    "\n",
    "# ---------------------------\n",
    "# 엣지 관련\n",
    "# ---------------------------\n",
    "\n",
    "def pick_all_edges(W: np.ndarray) -> List[Tuple[int,int,float]]:\n",
    "    \"\"\"모든 비대각 & 비0 엣지를 (i<-j, weight) 리스트로 반환\"\"\"\n",
    "    n=W.shape[0]; out=[]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            w=W[i,j]\n",
    "            if w!=0.0:\n",
    "                out.append((i,j,w))  # i<-j\n",
    "    # 정렬 기준: |w| 내림차순, 그 다음 w 내림차순\n",
    "    out.sort(key=lambda t:(abs(t[2]), t[2]), reverse=True)\n",
    "    return out\n",
    "\n",
    "def pick_top_k_edges(edge_list: List[Tuple[int,int,float]], k: int):\n",
    "    return edge_list[:k]\n",
    "\n",
    "def build_feature_df_from_edges(X_base: pd.DataFrame, cols: List[str],\n",
    "                                edges_ijw: List[Tuple[int,int,float]],\n",
    "                                feature_type: str) -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "    \"\"\"\n",
    "    주어진 edges_ijw(i,j,w)로부터 파생특징 DataFrame 생성.\n",
    "    반환: (X_feat, used_edges_meta)\n",
    "      - used_edges_meta: [{rank, src, dst, w, feature_name}]\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    used_meta = []\n",
    "    for rank, (i, j, w) in enumerate(edges_ijw, start=1):\n",
    "        A, B = cols[j], cols[i]  # j -> i (A=src, B=dst)\n",
    "        if feature_type == \"m\":\n",
    "            fname = f\"{A}_mul_{B}\"\n",
    "            feats[fname] = (X_base[A] * X_base[B]).values\n",
    "        elif feature_type == \"mw\":\n",
    "            fname = f\"{A}_mulw_{B}\"\n",
    "            feats[fname] = (w * (X_base[A] * X_base[B])).values\n",
    "        else:\n",
    "            raise ValueError(\"feature_type must be 'm' or 'mw'\")\n",
    "        used_meta.append(dict(rank=rank, src=A, dst=B, weight=float(w), feature=fname))\n",
    "    return pd.DataFrame(feats, index=X_base.index), used_meta\n",
    "\n",
    "def save_edges_csv(used_meta: List[Dict], feature_type: str, k: int):\n",
    "    ensure_dirs()\n",
    "    path = Path(f\"./reports/edges/{feature_type}_k{k}.csv\")\n",
    "    pd.DataFrame(used_meta).to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "def save_master_edges(all_edges_meta: List[Dict]):\n",
    "    ensure_dirs()\n",
    "    path = Path(\"./reports/edges/master_edges.csv\")\n",
    "    pd.DataFrame(all_edges_meta).to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 사용자 스타일 Wide CSV\n",
    "# ---------------------------\n",
    "\n",
    "def save_user_style_wide_csv(all_rows: List[Dict], out_path: Path):\n",
    "    groups=[\"O\",\"F\",\"OF\",\"F-O\",\"OF-O\"]; mets=[\"f1\",\"AUPRC\",\"AUROC\",\"Brier\",\"ECE\"]\n",
    "    key_map={}\n",
    "    for r in all_rows:\n",
    "        if r[\"set\"] not in {\"O\",\"F\",\"OF\"}: continue\n",
    "        key=(r[\"dag\"], r[\"model\"], r[\"feature\"], r[\"k\"])\n",
    "        if key not in key_map:\n",
    "            base=dict(DAG=r[\"dag\"],Model=r[\"model\"],Feature=r[\"feature\"],k=r[\"k\"])\n",
    "            for g in groups:\n",
    "                for m in mets: base[f\"{g}:{m}\"]=np.nan\n",
    "            key_map[key]=base\n",
    "        for m in mets: key_map[key][f\"{r['set']}:{m}\"]=r[m]\n",
    "    for rec in key_map.values():\n",
    "        for m in mets:\n",
    "            if pd.notna(rec[f\"F:{m}\"]) and pd.notna(rec[f\"O:{m}\"]):\n",
    "                rec[f\"F-O:{m}\"]=rec[f\"F:{m}\"]-rec[f\"O:{m}\"]\n",
    "            if pd.notna(rec[f\"OF:{m}\"]) and pd.notna(rec[f\"O:{m}\"]):\n",
    "                rec[f\"OF-O:{m}\"]=rec[f\"OF:{m}\"]-rec[f\"O:{m}\"]\n",
    "    wide=pd.DataFrame(list(key_map.values()))\n",
    "    order=[\"DAG\",\"Model\",\"Feature\"]+[f\"{g}:{m}\" for g in groups for m in mets]+[\"k\"]\n",
    "    wide=wide[order]\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    wide.to_csv(out_path, index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 데이터 로드 → NOTEARS → OLS W\n",
    "# ---------------------------\n",
    "\n",
    "ensure_dirs()\n",
    "path = Path(DATA_PATH); assert path.exists(), f\"not found: {path}\"\n",
    "df = pd.read_csv(path); assert LABEL_COL in df.columns, f\"'{LABEL_COL}' not found\"\n",
    "id_cols = [c for c in df.columns if c.startswith(ID_PREFIX)]\n",
    "feature_cols = [c for c in df.columns if c not in id_cols + [LABEL_COL]]\n",
    "X_base = df[feature_cols].copy()\n",
    "y = df[LABEL_COL].astype(int).copy()\n",
    "\n",
    "# 분산0 제거\n",
    "var0 = [c for c in feature_cols if np.isclose(X_base[c].var(ddof=0), 0.0)]\n",
    "cols_used = [c for c in feature_cols if c not in var0]\n",
    "\n",
    "# 표준화\n",
    "Z = StandardScaler().fit_transform(X_base[cols_used].values.astype(float))\n",
    "\n",
    "# NOTEARS(linear)\n",
    "from notears.linear import notears_linear\n",
    "NOTEARS_L1 = 0.10     # sparsity 제어(높을수록 간선↓)\n",
    "NOTEARS_TH = 1e-3     # 간선 임계치\n",
    "\n",
    "B = notears_linear(Z, lambda1=NOTEARS_L1, loss_type='l2')  # B[i,j] : j->i\n",
    "B = np.asarray(B, dtype=float)\n",
    "\n",
    "# 구조 A 결정(임계치 적용)\n",
    "A = (np.abs(B) > NOTEARS_TH).astype(float)\n",
    "\n",
    "# OLS 재적합으로 W 안정화\n",
    "p = B.shape[0]\n",
    "W = np.zeros_like(B)\n",
    "for i in range(p):\n",
    "    parents = np.where(A[i, :] == 1.0)[0]\n",
    "    if parents.size == 0:\n",
    "        continue\n",
    "    yi = Z[:, i]\n",
    "    Xi = Z[:, parents]\n",
    "    coef, *_ = np.linalg.lstsq(Xi, yi, rcond=None)\n",
    "    W[i, parents] = coef\n",
    "\n",
    "# 후보 엣지 전부 추출 (|w| 내림차순)\n",
    "edge_candidates = pick_all_edges(W)  # [(i,j,w), ...]  i<-j\n",
    "Kmax = len(edge_candidates)          # 파생 엣지(파생 컬럼) 기준의 최대 개수\n",
    "print(\"candidate edges (derived features count) =\", Kmax)\n",
    "\n",
    "# 마스터 엣지 목록 저장 (분석용)\n",
    "master_meta = []\n",
    "for rank,(i,j,w) in enumerate(edge_candidates, start=1):\n",
    "    master_meta.append(dict(rank=rank, src=cols_used[j], dst=cols_used[i], weight=float(w)))\n",
    "save_master_edges(master_meta)\n",
    "\n",
    "# ---------------------------\n",
    "# k=1..Kmax 스윕, 모델 학습/저장\n",
    "# ---------------------------\n",
    "\n",
    "models = make_models()\n",
    "long_rows=[]; wide_rows=[]\n",
    "\n",
    "for k in range(1, Kmax+1):\n",
    "    # 상위 k개 엣지 선택\n",
    "    edges_k = pick_top_k_edges(edge_candidates, k)\n",
    "\n",
    "    # 파생세트: m, mw (m+mw 제거)\n",
    "    Xm, edges_meta_m   = build_feature_df_from_edges(X_base[cols_used], cols_used, edges_k, feature_type=\"m\")\n",
    "    Xmw, edges_meta_mw = build_feature_df_from_edges(X_base[cols_used], cols_used, edges_k, feature_type=\"mw\")\n",
    "\n",
    "    # 각 k마다 어떤 엣지를 썼는지 CSV로 저장\n",
    "    save_edges_csv(edges_meta_m,   \"m\",  k)\n",
    "    save_edges_csv(edges_meta_mw,  \"mw\", k)\n",
    "\n",
    "    feature_sets=[(\"original\", X_base), (\"m\", Xm), (\"mw\", Xmw)]\n",
    "    for mname, model in models:\n",
    "        for fname, Xf in feature_sets:\n",
    "            # original은 O만, 파생은 F/OF 평가\n",
    "            sets = [(\"O\", X_base)] if fname==\"original\" else [(\"F\", Xf), (\"OF\", pd.concat([X_base, Xf], axis=1))]\n",
    "            for sname, Xset in sets:\n",
    "                t0=time.time()\n",
    "                clf = (LGBMClassifier(**LGBM_PARAMS) if (_HAS_LGBM and mname==\"LightGBM\") else model)\n",
    "                metrics=run_cv(clf, Xset, y, n_splits=N_SPLITS, seed=RANDOM_STATE)\n",
    "                secs=time.time()-t0\n",
    "                if VERBOSE_PRINT: print(one_line(DAG_NAME,k,mname,sname,metrics,secs))\n",
    "                append_log_csv(dict(dag=DAG_NAME,k=k,model=mname,set=sname,feature=fname,**metrics,seconds=round(secs,3)))\n",
    "                long_rows.append(dict(DAG=DAG_NAME,k=k,Model=mname,Feature=fname,Set=sname,**metrics))\n",
    "                wide_rows.append(dict(dag=DAG_NAME,k=k,model=mname,feature=fname,set=sname,**metrics))\n",
    "\n",
    "# 리포트 저장\n",
    "df_long=pd.DataFrame(long_rows)\n",
    "df_long.to_csv(\"./reports/report_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "pv=df_long.pivot_table(index=[\"DAG\",\"k\",\"Model\",\"Feature\"], columns=\"Set\",\n",
    "                       values=[\"f1\",\"AUPRC\",\"AUROC\",\"Brier\",\"ECE\"], aggfunc=\"mean\")\n",
    "pv.to_csv(\"./reports/report_pivot.csv\", encoding=\"utf-8-sig\")\n",
    "save_user_style_wide_csv(wide_rows, Path(\"./reports/report_all_like_user.csv\"))\n",
    "print(\"saved reports\")\n",
    "\n",
    "# 모델별 최적(우선순위: AUROC desc, AUPRC desc, f1 desc, Brier asc, ECE asc)\n",
    "order_cols=[\"AUROC\",\"AUPRC\",\"f1\",\"Brier\",\"ECE\"]\n",
    "ascending =[False,   False,   False, True,   True]\n",
    "best_df=(df_long.sort_values(order_cols, ascending=ascending)\n",
    "               .groupby(\"Model\", as_index=False).head(1).reset_index(drop=True))\n",
    "best_df.to_csv(\"./reports/best_per_model.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "best_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
