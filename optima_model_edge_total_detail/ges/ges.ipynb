{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b75ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 라이브러리 및 설정  (변경: GES 사용, DAG_NAME='GES')\n",
    "import warnings, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    _HAS_LGBM = True\n",
    "except Exception:\n",
    "    _HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    _HAS_XGB = True\n",
    "except Exception:\n",
    "    _HAS_XGB = False\n",
    "\n",
    "# GES 우선, 미설치 시 점수기반 대체(HillClimbSearch)\n",
    "_HAS_GES = True\n",
    "try:\n",
    "    from causallearn.search.ScoreBased.GES import ges\n",
    "except Exception:\n",
    "    _HAS_GES = False\n",
    "\n",
    "_HAS_HC = True\n",
    "try:\n",
    "    from pgmpy.estimators import HillClimbSearch, BicScore\n",
    "except Exception:\n",
    "    _HAS_HC = False\n",
    "\n",
    "DATA_PATH = \"./training_data.csv\"\n",
    "LABEL_COL = \"label\"\n",
    "ID_PREFIX = \"Unnamed\"\n",
    "N_SPLITS = 10\n",
    "RANDOM_STATE = 42\n",
    "DAG_NAME = \"GES\"\n",
    "CLASS_WEIGHT_BALANCED = True\n",
    "VERBOSE_PRINT = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3413207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 파라미터와 유틸\n",
    "LGBM_PARAMS = dict(\n",
    "    n_estimators=300, learning_rate=0.05, num_leaves=63, max_depth=-1,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, reg_alpha=0.0,\n",
    "    objective=\"binary\", class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_lambda=1.0, reg_alpha=0.0, objective=\"binary:logistic\", eval_metric=\"auc\",\n",
    "    random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "def ensure_dirs():\n",
    "    Path(\"./logs\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"./reports\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"./reports/edges\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"./reports/edges/by_model\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    ece=0.0; N=len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        L,R = bins[b], bins[b+1]\n",
    "        m = (y_prob>=L)&(y_prob<(R if b<n_bins-1 else R+1e-12))\n",
    "        if not np.any(m): continue\n",
    "        ece += (m.sum()/N)*abs(y_true[m].mean()-y_prob[m].mean())\n",
    "    return float(ece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6538a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 모델, 로깅, 평가\n",
    "def make_models():\n",
    "    ms=[(\"Logit\", LogisticRegression(solver=\"lbfgs\", max_iter=200,\n",
    "                                     class_weight=(\"balanced\" if CLASS_WEIGHT_BALANCED else None))),\n",
    "        (\"RandomForest\", RandomForestClassifier(n_estimators=400,\n",
    "                                               class_weight=(\"balanced\" if CLASS_WEIGHT_BALANCED else None),\n",
    "                                               n_jobs=-1, random_state=RANDOM_STATE))]\n",
    "    if _HAS_XGB:\n",
    "        ms.append((\"XGBoost\", XGBClassifier(**XGB_PARAMS, verbosity=0)))\n",
    "    if _HAS_LGBM:\n",
    "        ms.append((\"LightGBM\", LGBMClassifier(**LGBM_PARAMS)))\n",
    "    return ms\n",
    "\n",
    "def append_log_csv(row: Dict):\n",
    "    ensure_dirs()\n",
    "    p=Path(\"./logs/run_log.csv\")\n",
    "    pd.DataFrame([row]).to_csv(p, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=not p.exists())\n",
    "\n",
    "def one_line(dag,k,model,set_name,met,secs):\n",
    "    return (f\"DAG={dag} | k={k} | model={model} | set={set_name} | \"\n",
    "            f\"f1={met['f1']:.4f}, AUPRC={met['AUPRC']:.4f}, AUROC={met['AUROC']:.4f}, \"\n",
    "            f\"Brier={met['Brier']:.4f}, ECE={met['ECE']:.4f} | time={secs:.2f}s\")\n",
    "\n",
    "def run_cv(model, X, y, n_splits=10, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    f1s, auprcs, aurocs, briers, eces = [], [], [], [], []\n",
    "    Xv = X.values.astype(np.float32); yv = y.values.astype(int)\n",
    "    for tr, te in skf.split(Xv, yv):\n",
    "        X_tr, X_te, y_tr, y_te = Xv[tr], Xv[te], yv[tr], yv[te]\n",
    "        is_lgbm = (model.__class__.__name__ == \"LGBMClassifier\")\n",
    "        if is_lgbm:\n",
    "            X_tr2, X_val, y_tr2, y_val = train_test_split(X_tr, y_tr, test_size=0.2, random_state=seed, stratify=y_tr)\n",
    "            model.fit(X_tr2, y_tr2, eval_set=[(X_val, y_val)], eval_metric=\"auc\")\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "        proba = model.predict_proba(X_te)[:,1] if hasattr(model,\"predict_proba\") else model.predict(X_te).astype(float)\n",
    "        pred = (proba>=0.5).astype(int)\n",
    "        f1s.append(f1_score(y_te, pred, zero_division=0))\n",
    "        auprcs.append(average_precision_score(y_te, proba) if len(np.unique(y_te))>1 else np.nan)\n",
    "        aurocs.append(roc_auc_score(y_te, proba) if len(np.unique(y_te))>1 else np.nan)\n",
    "        briers.append(brier_score_loss(y_te, proba))\n",
    "        eces.append(expected_calibration_error(y_te, proba, 10))\n",
    "    return dict(f1=float(np.nanmean(f1s)), AUPRC=float(np.nanmean(auprcs)), AUROC=float(np.nanmean(aurocs)),\n",
    "                Brier=float(np.nanmean(briers)), ECE=float(np.nanmean(eces)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db706b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 엣지 관련 함수와 저장 함수\n",
    "def pick_all_edges(W: np.ndarray):\n",
    "    n=W.shape[0]; out=[]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            w=W[i,j]\n",
    "            if w!=0.0:\n",
    "                out.append((i,j,w))\n",
    "    out.sort(key=lambda t:(abs(t[2]), t[2]), reverse=True)\n",
    "    return out\n",
    "\n",
    "def pick_top_k_edges(edge_list, k):\n",
    "    return edge_list[:k]\n",
    "\n",
    "def build_feature_df_from_edges(X_base, cols, edges_ijw, feature_type):\n",
    "    feats = {}\n",
    "    used_meta = []\n",
    "    for rank, (i, j, w) in enumerate(edges_ijw, start=1):\n",
    "        A, B = cols[j], cols[i]\n",
    "        if feature_type == \"m\":\n",
    "            fname = f\"{A}_mul_{B}\"\n",
    "            feats[fname] = (X_base[A] * X_base[B]).values\n",
    "        elif feature_type == \"mw\":\n",
    "            fname = f\"{A}_mulw_{B}\"\n",
    "            feats[fname] = (w * (X_base[A] * X_base[B])).values\n",
    "        used_meta.append(dict(rank=rank, src=A, dst=B, weight=float(w), feature=fname))\n",
    "    return pd.DataFrame(feats, index=X_base.index), used_meta\n",
    "\n",
    "def save_edges_csv(used_meta, feature_type, k):\n",
    "    ensure_dirs()\n",
    "    path = Path(f\"./reports/edges/{feature_type}_k{k}.csv\")\n",
    "    pd.DataFrame(used_meta).to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "def save_master_edges(all_edges_meta: List[Dict]):\n",
    "    ensure_dirs()\n",
    "    path = Path(\"./reports/edges/master_edges.csv\")\n",
    "    pd.DataFrame(all_edges_meta).to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "def save_edges_by_model(dag: str, k: int, model: str, feature: str, set_name: str, used_meta: List[Dict]):\n",
    "    ensure_dirs()\n",
    "    df = pd.DataFrame(used_meta)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[\"rank\",\"src\",\"dst\",\"weight\",\"feature\"])\n",
    "    df.insert(0, \"Set\", set_name)\n",
    "    df.insert(0, \"Feature\", feature)\n",
    "    df.insert(0, \"Model\", model)\n",
    "    df.insert(0, \"k\", k)\n",
    "    df.insert(0, \"DAG\", dag)\n",
    "    per_run = Path(f\"./reports/edges/by_model/{model}_{feature}_{set_name}_k{k}.csv\")\n",
    "    df.to_csv(per_run, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    summary_row = dict(\n",
    "        DAG=dag, k=k, Model=model, Feature=feature, Set=set_name,\n",
    "        edge_count=len(used_meta),\n",
    "        features=\";\".join([m[\"feature\"] for m in used_meta]) if used_meta else \"\"\n",
    "    )\n",
    "    summary_p = Path(\"./reports/edges/usage_summary.csv\")\n",
    "    pd.DataFrame([summary_row]).to_csv(summary_p, mode=\"a\", index=False, encoding=\"utf-8-sig\", header=not summary_p.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e9914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge (derived feature) count = 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: 구조학습을 GES로 수행 (미설치 시 HillClimbSearch로 대체), 이후 OLS로 W 추정\n",
    "ensure_dirs()\n",
    "path = Path(DATA_PATH); assert path.exists(), f\"not found: {path}\"\n",
    "df = pd.read_csv(path); assert LABEL_COL in df.columns, f\"'{LABEL_COL}' not found\"\n",
    "id_cols = [c for c in df.columns if c.startswith(ID_PREFIX)]\n",
    "feature_cols = [c for c in df.columns if c not in id_cols + [LABEL_COL]]\n",
    "X_base = df[feature_cols].copy()\n",
    "y = df[LABEL_COL].astype(int).copy()\n",
    "\n",
    "var0 = [c for c in feature_cols if np.isclose(X_base[c].var(ddof=0), 0.0)]\n",
    "cols_used = [c for c in feature_cols if c not in var0]\n",
    "Z = StandardScaler().fit_transform(X_base[cols_used].values.astype(float))\n",
    "p = len(cols_used)\n",
    "\n",
    "A = np.zeros((p, p), dtype=float)\n",
    "\n",
    "if _HAS_GES:\n",
    "    ges_res = ges(Z, score_func='local_score_BIC')\n",
    "    G = getattr(ges_res, \"G\", ges_res)\n",
    "    try:\n",
    "        A_tmp = np.zeros((p, p), dtype=float)\n",
    "        for i in range(p):\n",
    "            for j in range(p):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if G.is_directed_from_to(j, i):\n",
    "                    A_tmp[i, j] = 1.0\n",
    "        A = A_tmp\n",
    "    except Exception:\n",
    "        edges = getattr(G, \"get_graph_edges\", lambda: [])()\n",
    "        for e in edges:\n",
    "            try:\n",
    "                u = int(str(e.get_node1()))\n",
    "                v = int(str(e.get_node2()))\n",
    "                ep1 = str(e.get_endpoint1())\n",
    "                ep2 = str(e.get_endpoint2())\n",
    "                if ep1 == 'TAIL' and ep2 == 'ARROW':\n",
    "                    A[v, u] = 1.0\n",
    "                elif ep1 == 'ARROW' and ep2 == 'TAIL':\n",
    "                    A[u, v] = 1.0\n",
    "            except Exception:\n",
    "                continue\n",
    "elif _HAS_HC:\n",
    "    df_std = pd.DataFrame(Z, columns=cols_used)\n",
    "    est = HillClimbSearch(df_std, scoring_method=BicScore(df_std))\n",
    "    model = est.estimate(scoring_method=BicScore(df_std))\n",
    "    for u, v in model.edges():\n",
    "        iu = cols_used.index(u)\n",
    "        iv = cols_used.index(v)\n",
    "        A[iv, iu] = 1.0\n",
    "else:\n",
    "    raise ImportError(\"GES 실행에 필요한 라이브러리를 찾지 못했습니다. causal-learn 또는 pgmpy 설치 필요\")\n",
    "\n",
    "W = np.zeros((p, p), dtype=float)\n",
    "for i in range(p):\n",
    "    parents = np.where(A[i, :] == 1.0)[0]\n",
    "    if parents.size == 0:\n",
    "        continue\n",
    "    yi = Z[:, i]\n",
    "    Xi = Z[:, parents]\n",
    "    coef, *_ = np.linalg.lstsq(Xi, yi, rcond=None)\n",
    "    W[i, parents] = coef\n",
    "\n",
    "edge_candidates = []\n",
    "for i in range(p):\n",
    "    for j in range(p):\n",
    "        if i == j:\n",
    "            continue\n",
    "        w = W[i, j]\n",
    "        if w != 0.0:\n",
    "            edge_candidates.append((i, j, w))\n",
    "edge_candidates.sort(key=lambda t: (abs(t[2]), t[2]), reverse=True)\n",
    "\n",
    "Kmax = len(edge_candidates)\n",
    "print(\"edge (derived feature) count =\", Kmax)\n",
    "\n",
    "master_meta = []\n",
    "for rank, (i, j, w) in enumerate(edge_candidates, start=1):\n",
    "    master_meta.append(dict(rank=rank, src=cols_used[j], dst=cols_used[i], weight=float(w)))\n",
    "save_master_edges(master_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d78bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved report_long.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'f1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m df_long\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./reports/report_long.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved report_long.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m pv\u001b[38;5;241m=\u001b[39m\u001b[43mdf_long\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDAG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAUPRC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAUROC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBrier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mECE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m pv\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./reports/report_pivot.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_user_style_wide_csv\u001b[39m(all_rows: List[Dict], out_path: Path):\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:9509\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   9492\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9493\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   9494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9505\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   9506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9507\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[1;32m-> 9509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9510\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9514\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:148\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(i)\n\u001b[0;32m    150\u001b[0m to_filter \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;241m+\u001b[39m values:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'f1'"
     ]
    }
   ],
   "source": [
    "# Cell 6: 학습 루프와 리포트 (K=0 안전 처리 포함)\n",
    "models = make_models()\n",
    "long_rows=[]; wide_rows=[]\n",
    "\n",
    "# K 리스트: 엣지가 없으면 [0], 있으면 0..Kmax\n",
    "k_list = [0] if Kmax == 0 else list(range(0, Kmax+1))\n",
    "\n",
    "for k in k_list:\n",
    "    if k == 0:\n",
    "        # 엣지 없음: baseline(O)만 수행\n",
    "        edges_k = []\n",
    "        edges_meta_m, edges_meta_mw = [], []\n",
    "        feature_sets = [(\"original\", X_base)]\n",
    "    else:\n",
    "        # 상위 k개 엣지 기반 파생특징 생성\n",
    "        edges_k = pick_top_k_edges(edge_candidates, k)\n",
    "        Xm,  edges_meta_m  = build_feature_df_from_edges(X_base[cols_used], cols_used, edges_k, \"m\")\n",
    "        Xmw, edges_meta_mw = build_feature_df_from_edges(X_base[cols_used], cols_used, edges_k, \"mw\")\n",
    "        save_edges_csv(edges_meta_m,  \"m\",  k)\n",
    "        save_edges_csv(edges_meta_mw, \"mw\", k)\n",
    "        feature_sets = [(\"original\", X_base), (\"m\", Xm), (\"mw\", Xmw)]\n",
    "\n",
    "    for mname, model in models:\n",
    "        for fname, Xf in feature_sets:\n",
    "            # original은 O만, 파생은 F/OF\n",
    "            sets = [(\"O\", X_base)] if fname==\"original\" else [(\"F\", Xf), (\"OF\", pd.concat([X_base, Xf], axis=1))]\n",
    "\n",
    "            # 이번 조합에서 사용된 엣지 메타(파일 기록용)\n",
    "            if fname == \"original\":\n",
    "                used_meta_for_this_run = []\n",
    "            elif fname == \"m\":\n",
    "                used_meta_for_this_run = edges_meta_m\n",
    "            else:  # \"mw\"\n",
    "                used_meta_for_this_run = edges_meta_mw\n",
    "\n",
    "            for sname, Xset in sets:\n",
    "                # 모델별·세트별·k별 엣지 사용 내역 저장\n",
    "                save_edges_by_model(DAG_NAME, k, mname, fname, sname, used_meta_for_this_run)\n",
    "\n",
    "                # 학습\n",
    "                t0=time.time()\n",
    "                clf = (LGBMClassifier(**LGBM_PARAMS) if (_HAS_LGBM and mname==\"LightGBM\") else model)\n",
    "                metrics=run_cv(clf, Xset, y, n_splits=N_SPLITS, seed=RANDOM_STATE)\n",
    "                secs=time.time()-t0\n",
    "\n",
    "                if VERBOSE_PRINT:\n",
    "                    print(one_line(DAG_NAME,k,mname,sname,metrics,secs))\n",
    "\n",
    "                # 로그/롱 포맷 축적\n",
    "                append_log_csv(dict(dag=DAG_NAME,k=k,model=mname,set=sname,feature=fname,**metrics,seconds=round(secs,3)))\n",
    "                long_rows.append(dict(DAG=DAG_NAME,k=k,Model=mname,Feature=fname,Set=sname,**metrics))\n",
    "                wide_rows.append(dict(dag=DAG_NAME,k=k,model=mname,feature=fname,set=sname,**metrics))\n",
    "\n",
    "# report_long 저장\n",
    "df_long=pd.DataFrame(long_rows)\n",
    "df_long.to_csv(\"./reports/report_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"saved report_long.csv\")\n",
    "\n",
    "# df_long이 비어있지 않을 때만 피벗/리포트 생성\n",
    "if not df_long.empty and all(col in df_long.columns for col in [\"DAG\",\"k\",\"Model\",\"Feature\",\"Set\",\"f1\",\"AUPRC\",\"AUROC\",\"Brier\",\"ECE\"]):\n",
    "    pv=df_long.pivot_table(index=[\"DAG\",\"k\",\"Model\",\"Feature\"], columns=\"Set\",\n",
    "                           values=[\"f1\",\"AUPRC\",\"AUROC\",\"Brier\",\"ECE\"], aggfunc=\"mean\")\n",
    "    pv.to_csv(\"./reports/report_pivot.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "    def save_user_style_wide_csv(all_rows: List[Dict], out_path: Path):\n",
    "        groups=[\"O\",\"F\",\"OF\",\"F-O\",\"OF-O\"]; mets=[\"f1\",\"AUPRC\",\"AUROC\",\"Brier\",\"ECE\"]\n",
    "        key_map={}\n",
    "        for r in all_rows:\n",
    "            if r[\"set\"] not in {\"O\",\"F\",\"OF\"}: continue\n",
    "            key=(r[\"dag\"], r[\"model\"], r[\"feature\"], r[\"k\"])\n",
    "            if key not in key_map:\n",
    "                base=dict(DAG=r[\"dag\"],Model=r[\"model\"],Feature=r[\"feature\"],k=r[\"k\"])\n",
    "                for g in groups:\n",
    "                    for m in mets: base[f\"{g}:{m}\"]=np.nan\n",
    "                key_map[key]=base\n",
    "            for m in mets: key_map[key][f\"{r['set']}:{m}\"]=r[m]\n",
    "        for rec in key_map.values():\n",
    "            for m in mets:\n",
    "                if pd.notna(rec[f\"F:{m}\"]) and pd.notna(rec[f\"O:{m}\"]):\n",
    "                    rec[f\"F-O:{m}\"]=rec[f\"F:{m}\"]-rec[f\"O:{m}\"]\n",
    "                if pd.notna(rec[f\"OF:{m}\"]) and pd.notna(rec[f\"O:{m}\"]):\n",
    "                    rec[f\"OF-O:{m}\"]=rec[f\"OF:{m}\"]-rec[f\"O:{m}\"]\n",
    "        wide=pd.DataFrame(list(key_map.values()))\n",
    "        order=[\"DAG\",\"Model\",\"Feature\"]+[f\"{g}:{m}\" for g in groups for m in mets]+[\"k\"]\n",
    "        wide=wide[order]\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        wide.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    save_user_style_wide_csv(wide_rows, Path(\"./reports/report_all_like_user.csv\"))\n",
    "    print(\"saved report_pivot.csv and report_all_like_user.csv\")\n",
    "\n",
    "    order_cols=[\"AUROC\",\"AUPRC\",\"f1\",\"Brier\",\"ECE\"]\n",
    "    ascending =[False,   False,   False, True,   True]\n",
    "    best_df=(df_long.sort_values(order_cols, ascending=ascending)\n",
    "                    .groupby(\"Model\", as_index=False).head(1).reset_index(drop=True))\n",
    "    best_df.to_csv(\"./reports/best_per_model.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    best_df\n",
    "else:\n",
    "    print(\"warning: df_long is empty or missing metric columns. Skipping pivot and best-per-model reports.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
